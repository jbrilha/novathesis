%!TEX root = ../../template.tex

\section{Lightweight Data Compression and Energy Efficiency}
\label{sec:data_compression}

\todo{TOFIX: não fiz nada em relação ao flow fraquito desta section porque
provavelmente vou compactar muito mais. thoughts?}

Energy constraints present one of the most fundamental limitations in
battery-powered \gls{iot} deployments, particularly in hazardous deployments where
replacements and recharging are impractical, and during disaster scenarios
where grid power likely becomes unavailable.

While multi-protocol communication and decentralized coordination efforts
provide resilience, they also introduce increased energy costs through frequent
transmissions, protocol adaptation overhead and forwarding costs.

Data transmission dominates energy consumption across wireless technologies:
\gls{LoRa} and SIGFOX can represent up to 99.9\% of power consumption for
transmission, while even shorter-range technologies like \gls{BLE} and IEEE 802.15.4
dedicate 85-90\% of their energy budget to radio
operations~\cite{ambrosia2021}.

This reality makes data reduction techniques essential for extending
operational lifetime in infrastructure-independent scenarios. However,
compression itself consumes energy through additional computational work, thus
creating an unavoidable tug-of-war between compression overhead and
transmission benefits.

\subsection{Prediction-Based Data Reduction}
\label{subsec:prediction_reduction}

The Ambrosia protocol~\cite{ambrosia2021} demonstrates that lightweight
prediction can achieve substantial data reductions on resource-constrained
devices. The core idea is that not all sensor readings need to be transmitted
if their values can be accurately predicted at the server (within the bounds
of a specified error threshold), provided that both sender and receiver
maintain a synchronized "prediction state".

Their approach uses window-based forecasting where the next sample is predicted
by adding the average differences between recent previous samples, making it
dramatically lighter than sophisticated time-series forecasting techniques like
\gls{ARIMA}~\cite{arima2017} while still achieving comparable data reduction
performance.

The protocol works by sending the first \emph{w} (window size) true samples
collected by the sensor node to the server, and for every sample after that
comparing its true value to the one predicted locally; the true sample value is
sent to the server only if the absolute difference between it and the prediction
is greater than a user or application-specified error threshold $\delta$.

The same simple prediction scheme is used on both ends of this communication,
and the predicted value is the one used for further predictions -- even on the
sensor node that knows the true value, to ensure consistency between the two
endpoints.

This design achieves up to 60\% data reduction with appropriate $\delta$
configuration while maintaining sufficient accuracy for diverse applications:
for error-tolerant use cases like anomaly detection, $\delta$ values will be
higher to allow for reasonable fluctuations, thus enabling more significant
data reductions, while error-sensitive applications requiring higher precision
will naturally lean towards stricter error thresholds, but still benefitting
from some amount of data reduction.

The major takeaway from this work is that the window-based prediction executed
99\% faster than \gls{ARIMA} forecasting in their evaluation, making it viable
for streaming sensor data at high rates. The approach achieved 2$\times$
battery lifetime improvement in high-traffic scenarios and demonstrated
compatibility with extremely constrained devices (livestock ear tags with Atmel
8-bit microcontrollers).

\subsection{Lightweight Compression}
\label{subsec:lightweight_compression}

While prediction-based reduction minimizes the number of transmissions,
compression techniques are essential to reduce the size of the data that
\emph{must} be transmitted.

A few complementary approaches address different considerations and
constraints: hybrid schemes balance accuracy with transmission costs through
lossy/lossless models; lossless time-series compression exploits the natural
temporal relation in continuous sensor readings; and two-tier architectures
apply different techniques at both the sensor and gateway level to reduce
energy costs.

\subsubsection*{Hybrid Lossy/Lossless Compression}

The hybrid compression scheme presented in~\cite{hybrid2017} addresses accuracy
requirements by separating real-time lossy transmission from on-demand lossless
reconstruction. The Fan algorithm adaptively sub-samples data maintaining
bounded error $\varepsilon$, achieving average 7.8$\times$ and 2.1$\times$
\gls{CR} for lossy and lossless compression, respectively.

The approach provides graceful degradation through battery-aware switching, in
that when battery drops below a certain threshold (e.g., 20\%), the system
switches to lossy-only transmission instead of lossless, extending lifetime at
the cost of reconstruction accuracy.

This trade-off aligns with disaster-scenario requirements where sustained
operation during prolonged periods of network outages often takes higher
priority over perfect data-fidelity, as approximate sensor readings over
extended periods of time provide greater situational awareness than
high-precision measurements from devices that quickly deplete their batteries.
However, safety-critical sensors (e.g., structural integrity sensors, hazardous
gas detectors) may require different thresholds (if any) or other policies to
balance longevity with accuracy requirements.

\subsubsection*{Lossless Time-Series Compression}

Sprintz~\cite{sprintz2018} targets lossless compression for multivariate
integer time-series with extreme resource constraints, achieving 2-10$\times$
compression ratios with <1KB memory footprint and 8-sample block sizes. The
four-component algorithm combines forecasting (delta coding or \gls{FIRE}
online learning), bit-packing with zigzag encoding, \gls{RLE} for all-zero
blocks, and optional Huffman coding.

Delta coding is extremely fast, and when combined with \gls{RLE} becomes even
more so, as it yields a run of zero errors if the data is constant, which is
likely to happen for nominal sensor readings. \gls{FIRE} forecasting yields
better compression, but its online learning approach place it beyond the scope
of this paper.

A \emph{forescaster} is employed to predict each sample based on previous ones,
and the difference between the next and the predicted sample are encoded. This
difference is typically closer to zero than the next sample itself.

Any prediction errors from the previous forecasting step are zigzag
encoded as a "payload",
and a header is prepended with sufficient information to invert the bit
packing.

If a block happens to consist only of zeros, this approach waits for a block in
which there is a non-zero error, and the number of all-zero blocks are written
out using \gls{RLE} instead of the (empty) payload.

Finally, the bit packed representation of each block can be \emph{entropy
	coded} using a Huffman coder, applied to the headers and payloads. This is done
after bit packing because not only is it faster, but it also increases
compression.

Decompression can achieve up to 3GB/s throughput without Huffman coding
(>500MB/s with Huffman) in a single thread, enabling high-speed data retrieval
on gateway-tier devices. Compression maintains >200MB/s on 8-bit data,
sufficient for real-time operation even on embedded platforms.

\subsubsection*{Two-Tier Compression Architecture}

The two-tier data reduction technique proposed in~\cite{twotier2019}
applies compression at both sensor nodes (Tier 1) and gateways (Tier 2) to
reduce energy consumption in the overall system.

Sensor nodes employ Delta encoding followed by \gls{RLE}, allowing this
approach to achieve 80-84\% compression by exploiting the temporal correlation
in sensor data. Gateways perform hierarchical clustering based on the \gls{MDL}
principle -- 
\todo{FIXED: descrever o MDL um bocado}
that is, combining pairs of datasets into one cluster when their
combined representation is shorter than separate encodings -- to transmit
\emph{hypothesis} data sets and difference vectors instead of full data sets.

The Delta+\gls{RLE} implementation -- validated through OMNeT++ simulation --
proves to be lightweight enough for resource-constrained nodes, while gateway
clustering reduces transmission to cloud infrastructure.

% \subsection{Receiver-Side Energy Considerations}
% \label{subsec:receiver_energy}
%
% \todo{TOFIX: outra vez o paper do MDPI.... retirar por completo, fingir que não vi?}
% An often overlooked aspect of \gls{iot} energy optimization is the asymmetry
% between sender and receiver energy consumption. The adaptive protocol selection
% framework analysis~\cite{adaptive2025} revealed that receiver nodes
% consistently consume 15-20\% more energy than senders across all evaluated
% protocols (\gls{MQTT}, \gls{CoAP}, \gls{HTTP}).
%
% This asymmetry shifts the energy bottleneck from endpoint sensors to
% intermediary nodes like gateways and has direct implications for gateway power
% management in disaster scenarios where battery replacement becomes impossible.
% The increased receiver cost stems from the added computational work required
% for parsing, processing, and managing incoming messages, particularly at high
% message rates.
%
% For \gls{ubabel}'s proposed heterogeneous architecture where sensor
% nodes might eventually forward data to aggregators/gateways, this finding has
% direct implications in that managing the power consumption of these gateways
% becomes as significant as sensor power optimization, since one gateway might
% receive substantial data from several nodes even if individually they don't
% perform frequent transmissions.

\subsection{Discussion}
\label{subsec:rw_discussion_5}

Existing data reduction techniques demonstrate substantial energy savings on 
resource-constrained platforms but assume single-protocol 
deployments.

Ambrosia's~\cite{ambrosia2021} window-based prediction achieves 60\% data
reduction with 99\% faster execution than \gls{ARIMA} while maintaining
accuracy within application-specific error thresholds, validated on 8-bit
microcontrollers.

Sprintz~\cite{sprintz2018} provides lossless time-series compression with
2-10$\times$ compression ratios using <1KB memory and 8-sample blocks,
combining delta coding, zigzag encoding, \gls{RLE}, and optional Huffman coding
with >200MB/s compression throughput suitable for real-time embedded operation.

Hybrid lossy/lossless~\cite{hybrid2017} schemes enable graceful battery-aware
degradation, while two-tier architectures~\cite{twotier2019} (Delta+\gls{RLE}
at sensors, \gls{MDL} clustering at gateways) achieve 80-84\% compression
exploiting temporal correlation.\\

None of these approaches address protocol heterogeneity by themselves, and
different wireless technologies have distinct energy costs and range/accuracy
trade-offs that warrant protocol-specific compression/reduction strategies.

We have found no work that integrates prediction-based reduction with
protocol-aware adaptive thresholding and compression tuning for heterogeneous
multi-protocol networks.

\gls{ubabel}'s approach (Section~\ref{component:reduction_compression}) should,
therefore, consider lightweight prediction and compression in multi-protocol
scenarios with per-protocol thresholds and strategy selection.
