\section{Lightweight Data Compression and Energy Efficiency}
\label{sec:data_compression}

Energy constraints present one of the most fundamental limitations in
battery-powered \gls{iot} deployments, particularly in hazardous deployments where
replacements and recharging are impractical, and during disaster scenarios
where grid power likely becomes unavailable.

While multi-protocol communication and decentralized coordination efforts
provide resilience, they also introduce increased energy costs through frequent
transmissions, protocol adaptation overhead and forwarding costs.

Data transmission dominates energy consumption across wireless technologies:
\gls{LoRa} and SIGFOX can represent up to 99.9\% of power consumption for
transmission, while even shorter-range technologies like \gls{BLE} and IEEE 802.15.4
dedicate 85-90\% of their energy budget to radio
operations~\cite{ambrosia2021}.

This reality makes data reduction techniques essential for extending
operational lifetime in infrastructure-independent scenarios. However,
compression itself consumes energy through additional computational work, thus
creating an unavoidable tug-of-war between compression overhead and
transmission benefits.

\subsection{Prediction-Based Data Reduction}
\label{subsec:prediction_reduction}

The Ambrosia protocol~\cite{ambrosia2021} demonstrates that lightweight
prediction can achieve substantial data reductions on resource-constrained
devices. The core idea is that not all sensor readings need to be transmitted
if their values can be accurately predicted at the server (within the bounds
of a specified error threshold), provided that both sender and receiver
maintain a synchronized "prediction state".

Their approach uses window-based forecasting where the next sample is predicted
by adding the average differences between recent previous samples, making it
dramatically lighter than sophisticated time-series forecasting techniques like
\gls{ARIMA}~\cite{arima2017} while still achieving comparable data reduction
performance.

The protocol works by sending the first \emph{w} (window size) true samples
collected by the sensor node to the server, and for every sample after that
comparing its true value to the one predicted locally; the true sample value is
sent to the server only if the absolute difference between it and the prediction
is greater than a user or application-specified error threshold $\delta$.

The same simple prediction scheme is used on both ends of this communication,
and the predicted value is the one used for further predictions -- even on the
sensor node that knows the true value, to ensure consistency between the two
endpoints.

This design achieves up to 60\% data reduction with appropriate $\delta$
configuration while maintaining sufficient accuracy for diverse applications:
for error-tolerant use cases like anomaly detection, $\delta$ values will be
higher to allow for reasonable fluctuations, thus enabling more significant
data reductions, while error-sensitive applications requiring higher precision
will naturally lean towards stricter error thresholds, but still benefitting
from some amount of data reduction.

The major takeaway from this work is that the window-based prediction executed
99\% faster than \gls{ARIMA} forecasting in their evaluation, making it viable
for streaming sensor data at high rates. The approach achieved 2$\times$
battery lifetime improvement in high-traffic scenarios and demonstrated
compatibility with extremely constrained devices (livestock ear tags with Atmel
8-bit microcontrollers).

\subsection{Lightweight Compression}
\label{subsec:lightweight_compression}

While prediction-based reduction minimizes the number of transmissions,
compression techniques are essential to reduce the size of the data that
\emph{must} be transmitted.

A few complementary approaches address different considerations and
constraints: hybrid schemes balance accuracy with transmission costs through
lossy/lossless models; lossless time-series compression exploits the natural
temporal relation in continuous sensor readings; and two-tier architectures
apply different techniques at both the sensor and gateway level to reduce
energy costs.

\subsubsection*{Hybrid Lossy/Lossless Compression}

The hybrid compression scheme presented in~\cite{hybrid2017} addresses accuracy
requirements by separating real-time lossy transmission from on-demand lossless
reconstruction. The Fan algorithm adaptively sub-samples data maintaining
bounded error $\varepsilon$, achieving average 7.8$\times$ and 2.1$\times$
\gls{CR} for lossy and lossless compression, respectively.

The approach provides graceful degradation through battery-aware switching, in
that when battery drops below a certain threshold (e.g., 20\%), the system
switches to lossy-only transmission instead of lossless, extending lifetime at
the cost of reconstruction accuracy.

\inlinetodo{could add more detail here but need to read it more carefully}

\subsubsection*{Lossless Time-Series Compression}

Sprintz~\cite{sprintz2018} targets lossless compression for multivariate
integer time-series with extreme resource constraints, achieving 2-10$\times$
compression ratios with <1KB memory footprint and 8-sample block sizes. The
four-component algorithm combines forecasting (delta coding or \gls{FIRE}
online learning), bit packing with zigzag encoding~\inlinetodo{add citation
	here for zigzag or not really necessary?}, \gls{RLE} for all-zero blocks, and
optional Huffman coding.

Delta coding is extremely fast, and when combined with \gls{RLE} becomes even
more so, as it yields a run of zero errors if the data is constant, which is
likely to happen for nominal sensor readings. \gls{FIRE} forecasting yields
better compression, but its online learning approach place it beyond the scope
of this paper.

A \emph{forescaster} is employed to predict each sample based on previous ones,
and the difference between the next and the predicted sample are encoded. This
difference is typically closer to zero than the next sample itself.

Any prediction errors from the previous forecasting step are zigzag
encoded as a "payload",
and a header is prepended with sufficient information to invert the bit
packing.

If a block happens to consist only of zeros, this approach waits for a block in
which there is a non-zero error, and the number of all-zero blocks are written
out using \gls{RLE} instead of the (empty) payload.

Finally, the bit packed representation of each block can be \emph{entropy
	coded} using a Huffman coder, applied to the headers and payloads. This is done
after bit packing because not only is it faster, but it also increases
compression.

Decompression can achieve up to 3GB/s throughput without Huffman coding
(>500MB/s with Huffman) in a single thread, enabling high-speed data retrieval
on gateway-tier devices. Compression maintains >200MB/s on 8-bit data,
sufficient for real-time operation even on embedded platforms.

\subsubsection*{Two-Tier Compression Architecture}

The two-tier data reduction technique proposed in~\cite{twotier2019}
applies compression at both sensor nodes (Tier 1) and gateways (Tier 2) to
reduce energy consumption in the overall system.

Sensor nodes employ Delta encoding followed by \gls{RLE}, allowing this approach to
achieve 80-84\% compression by exploiting the temporal correlation in sensor
data. Gateways perform hierarchical clustering based on the \gls{MDL} principle, transmitting \emph{hypothesis} data sets and
difference vectors instead of full data sets.

\todo{falar mais do MDL ou como é gateway side not so important?}

The Delta+\gls{RLE} implementation -- validated through OMNeT++ simulation --
proves to be lightweight enough for resource-constrained nodes, while gateway
clustering reduces transmission to cloud infrastructure.

\subsection{Receiver-Side Energy Considerations}
\label{subsec:receiver_energy}

An oft-overlooked aspect of \gls{iot} energy optimization is the asymmetry between
sender and receiver energy consumption. The adaptive protocol selection
framework analysis~\cite{adaptive2025} revealed that receiver nodes
consistently consume 15-20\% more energy than senders across all evaluated
protocols (\gls{MQTT}, \gls{CoAP}\cite{coap2012}, \gls{HTTP}).

This asymmetry shifts the energy bottleneck from endpoint sensors to
intermediary nodes like gateways and has direct implications for gateway power
management in disaster scenarios where battery replacement becomes impossible.
The increased receiver cost stems from the added computational work required
for parsing, processing, and managing incoming messages, particularly at high
message rates.

For \textbf{\gls{ubabel}}'s proposed heterogeneous architecture where sensor
nodes might eventually forward data to aggregators/gateways, this finding has
direct implications in that managing the power consumption of these gateways
becomes as significant as sensor power optimization, since one gateway might
receive substantial data from several nodes even if individually they don't
perform frequent transmissions.

\subsection{Implications for \textbf{\gls{ubabel}}}
\label{subsec:microbabel_implications_4}

Existing data reduction techniques demonstrate substantial energy savings on 
resource-constrained platforms but assume single-protocol 
deployments.

Ambrosia's~\cite{ambrosia2021} window-based prediction achieves 60\% data
reduction with 99\% faster execution than \gls{ARIMA} while maintaining
accuracy within application-specific error thresholds, validated on 8-bit
microcontrollers.

Sprintz~\cite{sprintz2018} provides lossless time-series compression with
2-10$\times$ compression ratios using <1KB memory and 8-sample blocks,
combining delta coding, zigzag encoding, \gls{RLE}, and optional Huffman coding
with >200MB/s compression throughput suitable for real-time embedded operation.

Hybrid lossy/lossless~\cite{hybrid2017} schemes enable graceful battery-aware
degradation, while two-tier architectures~\cite{twotier2019} (Delta+\gls{RLE}
at sensors, \gls{MDL} clustering at gateways) achieve 80-84\% compression
exploiting temporal correlation.\\

None of these approaches address protocol heterogeneity by themselves, and
different wireless technologies have distinct energy costs and range/accuracy
trade-offs that warrant protocol-specific compression/reduction strategies.

No existing work integrates prediction-based reduction with protocol-aware
adaptive thresholding and compression tuning for heterogeneous multi-protocol
networks.

\textbf{\gls{ubabel}}'s approach (Section~\ref{subsec:microbabel_approach_4})
must extend lightweight prediction and compression to multi-protocol scenarios
with per-protocol threshold and compression strategy selection while accounting
for gateway receiver energy costs.

\subsection{MicroBabel's Approach}
\label{subsec:microbabel_approach_4}

Extending Ambrosia's lightweight prediction approach to multi-protocol
scenarios, \textbf{\gls{ubabel}} will reduce data transmission through
prediction-based filtering: nodes transmit sensor readings only when prediction
error exceeds configurable thresholds (similar to Ambrosia), with
protocol-aware tuning to balance accuracy against energy constraints across
heterogeneous protocols.

Building on prediction-based reduction, the goal is to apply Delta+\gls{RLE} compression
to the readings that are transmitted, combining both techniques for added
energy savings and reduced data volume over the air.

Following the two-tier architecture pattern, compression occurs at sensor nodes
before transmission, with just the first sample in each window being
transmitted with no compression applied.

Adaptive threshold selection will adjust $\delta$ based on both channel
characteristics and node state:
\todo{estes deltas vinham do ambrosia, fica confuso agora que falo de delta encoding?}
\begin{itemize}
	\item \textbf{High $\delta$ for \gls{LoRa}}: Tolerates larger prediction errors
	      to minimize expensive long-range transmissions, sending only when
	      predictions deviate significantly. These packets undergo
	      aggressive Delta+\gls{RLE} compression to minimize airtime costs.
	\item \textbf{Low $\delta$ for \gls{BLE}}: Requires tighter prediction accuracy
	      for cheap local exchanges, allowing for more frequent transmissions to
	      maintain precision. Lighter compression (delta encoding only)
	      is applied here for lower latency.
	\item \textbf{Battery-aware adaptation}: As node energy depletes, increased
	      $\delta$ across all channels reduces transmission frequency, extending
	      lifetime for critical messages. Compression at this point becomes more
	      lossy to further minimize transmitted data volume.
\end{itemize}

\textbf{Application-aware transmission} will distinguish between data types:
periodic sensor readings (temperature, humidity, light) use window-based
prediction with configurable $\delta$ thresholds, while discrete events
(emergency alerts, button presses, motion detection) transmit immediately
without prediction. For predictable streams, accuracy-critical applications
maintain low $\delta$, while trend monitoring can accept higher values to
reduce transmission frequency.

Compression strategies also vary by data type: environmental sensors can
tolerate more prominent Delta+\gls{RLE} compression given their high temporal
correlation, while critical alerts transmit with minimal (if any) compression
to reduce latency.

\textbf{Gateway energy management} accounts for the 15-20\% receiver energy
cost by implementing selective radio shutdown: gateways can disable specific
protocols during low-activity periods, waking periodically to check for
incoming sync beacons or when prompted via other protocols. Decompression
overhead in these nodes is minimal compared to transmission costs at
the sensor level.
\todo{entrar pela coisa de desligar os rádios ainda mais?}

The expectation is that the combination of lightweight prediction, adaptive
thresholding, and protocol-specific tuning together with Delta+\gls{RLE} compression
will provide substantial energy savings without requiring complex and
computationally intensive compression algorithms that would themselves consume
significant power.
